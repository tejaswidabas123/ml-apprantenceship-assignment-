{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Multi-Task Learning Expansion for Sentence Transformer\n",
    "\n",
    "Expanding the sentence transformer model to accommodate multi-task learning involves significant adjustments to the architecture. This setup allows the model to handle multiple NLP tasks simultaneously, leveraging shared representations to improve overall efficiency and performance. Here's how we have adapted the transformer to manage two distinct tasks:\n",
    "\n",
    "## Expanded Transformer Model Architecture\n",
    "The updated model architecture includes task-specific classification layers on top of a shared transformer backbone. This design allows the model to optimize for multiple objectives, enhancing its utility and flexibility.\n",
    "\n",
    "### Explanation of Architectural Choices and Advantages\n",
    "\n",
    "1. **Shared Transformer Backbone:**\n",
    "   - **Rationale:** The shared layers (embedding, positional encoding, and transformer blocks) process input data in a way that captures universal linguistic features, which are beneficial for any NLP task. This setup reduces redundancy and conserves computational resources.\n",
    "   - **Advantages:** Sharing lower layers across tasks allows the model to learn a more robust representation of the language, which can improve generalization across tasks due to shared learning signals.\n",
    "\n",
    "2. **Task-Specific Classifiers:**\n",
    "   - **Rationale:** After processing through shared layers, task-specific classifiers (sentiment and engagement classifiers) tailor the learned embeddings to particular objectives. Each classifier focuses on optimizing for its respective task, allowing for specialization where necessary.\n",
    "   - **Advantages:** This approach enables the model to be flexible and adaptable, capable of addressing the nuances of different tasks while maintaining the efficiency of a unified model structure. The use of separate classifiers ensures that task-specific features can be learned without interference, potentially enhancing accuracy on individual tasks.\n",
    "\n",
    "3. **Mean Pooling Strategy:**\n",
    "   - **Rationale:** Before passing the output to the classifiers, applying a mean pooling reduces the sequence of vectors to a single vector that captures the essence of the input across all positions. This is particularly useful for classification tasks, as it distills the entire input into a format suitable for making a single prediction per input.\n",
    "   - **Advantages:** Mean pooling simplifies the output while retaining critical information, making it easier for the classifiers to perform effectively. It ensures that all parts of the input contribute to the final decision, enhancing the model's ability to understand and utilize the full context of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./archive/netflix_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['content','score','thumbsUpCount','at']]\n",
    "\n",
    "df = df[~df['content'].isnull()]\n",
    "\n",
    "df = df.astype({\n",
    "    'score':'int16',\n",
    "    'thumbsUpCount':'int16'\n",
    "})\n",
    "\n",
    "df['at'] = pd.to_datetime(df['at']) \n",
    "df['at'] = df['at'].dt.strftime('%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    112730.000000\n",
       "mean         10.514681\n",
       "std         101.402747\n",
       "min           0.000000\n",
       "25%           0.000000\n",
       "50%           0.000000\n",
       "75%           1.000000\n",
       "max        8032.000000\n",
       "Name: thumbsUpCount, dtype: float64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['thumbsUpCount'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling\n",
    "\n",
    "generating labels out of ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_labeller(x):\n",
    "    if x > 3:\n",
    "        # 'Positive'\n",
    "        return 2\n",
    "    elif x <= 3 and x > 2:\n",
    "        # 'Neutral'\n",
    "        return 1\n",
    "    else:\n",
    "        # 'Negative'\n",
    "        return 0\n",
    "\n",
    "\n",
    "def user_engagement_labeller(x):\n",
    "    if x > 12:\n",
    "        # 'High'\n",
    "        return 2\n",
    "    elif x <= 12 and x >= 7:\n",
    "        # \"Moderate\"\n",
    "        return 1\n",
    "    else:\n",
    "        # \"Low\"\n",
    "        return 0\n",
    "\n",
    "\n",
    "df['sentiment'] = df['score'].apply(lambda x: sentiment_labeller(x))\n",
    "df['user_engagement'] = df['thumbsUpCount'].apply(lambda x: user_engagement_labeller(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'].to_csv('content_text.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer.train(files=[\"content_text.txt\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataframe: 100%|██████████| 112730/112730 [00:06<00:00, 16947.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [56, 12780, 1677, 20724, 6867, 5390, 1522, 70,...\n",
       "1    [46, 7805, 2049, 17, 1530, 1607, 46, 12, 82, 2...\n",
       "2    [2215, 2171, 1514, 2019, 1547, 1642, 1736, 192...\n",
       "3    [1725, 1502, 1480, 1692, 1584, 3381, 1514, 147...\n",
       "4    [1984, 1560, 1596, 4417, 1483, 1632, 10029, 23...\n",
       "Name: tokenized_content, dtype: object"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def tokenize(text):\n",
    "    return tokenizer.encode(text).ids\n",
    "\n",
    "tqdm.pandas(desc=\"Processing dataframe: \")\n",
    "df['tokenized_content'] = df['content'].progress_apply(lambda x: tokenize(str(x)))\n",
    "df['tokenized_content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['content','tokenized_content','sentiment','user_engagement','at']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Head Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert self.head_dim * heads == embed_size, \"Embed size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(attention / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_len=500):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, embed_size)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        _2i = torch.arange(0, embed_size, step=2).float()\n",
    "\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i/embed_size)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i/embed_size)))\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.encoding = self.encoding.to(self.device)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoding[:, :x.size(1), :].to(x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length, num_classes_sentiment, num_classes_engagement):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.positional_encoding = PositionalEncoding(embed_size, max_length)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(embed_size, heads, dropout, forward_expansion) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.sentiment_classifier = nn.Linear(embed_size, num_classes_sentiment)\n",
    "        \n",
    "        self.engagement_classifier = nn.Linear(embed_size, num_classes_engagement)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.dropout(self.word_embedding(x) + self.positional_encoding(x))\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, None)\n",
    "\n",
    "        out = out.mean(dim=1)  \n",
    "\n",
    "        sentiment_output = self.sentiment_classifier(out)\n",
    "        engagement_output = self.engagement_classifier(out)\n",
    "\n",
    "        return sentiment_output, engagement_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "batch_size = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vocab_size = tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparetion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataframe: 100%|██████████| 112730/112730 [00:01<00:00, 70043.40it/s]\n"
     ]
    }
   ],
   "source": [
    "def pad_sequences(vector, pad_idx, max_len):\n",
    "    padded = vector + [pad_idx] * (max_len - len(vector))\n",
    "    return padded[:max_len]\n",
    "\n",
    "pad_idx = tokenizer.token_to_id(\"[PAD]\")\n",
    "df['padded_tokens'] = df['tokenized_content'].progress_apply(lambda x: pad_sequences(x, pad_idx, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_tokens_tensor = torch.tensor(df['padded_tokens'].tolist()).to(device)\n",
    "\n",
    "sentiment_tensor = torch.tensor(df['sentiment'].values).to(device)\n",
    "user_engagement_tensor = torch.tensor(df['user_engagement'].values).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_data, test_data, train_labels_sentiment, test_labels_sentiment, train_labels_engagement, test_labels_engagement = train_test_split(\n",
    "    padded_tokens_tensor, sentiment_tensor, user_engagement_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(train_data, train_labels_sentiment, train_labels_engagement)\n",
    "test_dataset = TensorDataset(test_data, test_labels_sentiment, test_labels_engagement)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training And Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(model,model_name):\n",
    "    torch.save(model.state_dict(), f'{model_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, num_epochs, optimizer):\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accuracies_sentiment = []\n",
    "    test_accuracies_engagement = []\n",
    "\n",
    "    loss_fn_sentiment = nn.CrossEntropyLoss()\n",
    "    loss_fn_engagement = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        train_loss = 0\n",
    "\n",
    "        for inputs, sentiment_labels, engagement_labels in tqdm(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            sentiment_labels = sentiment_labels.to(device)\n",
    "            engagement_labels = engagement_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            sentiment_preds, engagement_preds = model(inputs)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss_sentiment = loss_fn_sentiment(sentiment_preds, sentiment_labels)\n",
    "            loss_engagement = loss_fn_engagement(\n",
    "                engagement_preds, engagement_labels)\n",
    "            loss = loss_sentiment + loss_engagement\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_losses.append(train_loss / len(train_data))\n",
    "\n",
    "        model.eval()\n",
    "        total_accuracy_sentiment = 0\n",
    "        total_accuracy_engagement = 0\n",
    "        test_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, sentiment_labels, engagement_labels in tqdm(test_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                sentiment_labels = sentiment_labels.to(device)\n",
    "                engagement_labels = engagement_labels.to(device)\n",
    "\n",
    "                sentiment_preds, engagement_preds = model(inputs)\n",
    "\n",
    "                loss_sentiment = loss_fn_sentiment(sentiment_preds, sentiment_labels)\n",
    "                loss_engagement = loss_fn_engagement(\n",
    "                    engagement_preds, engagement_labels)\n",
    "                loss = loss_sentiment + loss_engagement\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                total_accuracy_sentiment += accuracy(sentiment_preds, sentiment_labels)\n",
    "                total_accuracy_engagement += accuracy(\n",
    "                    engagement_preds, engagement_labels)\n",
    "\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        avg_accuracy_sentiment = total_accuracy_sentiment / len(test_loader)\n",
    "        avg_accuracy_engagement = total_accuracy_engagement / len(test_loader)\n",
    "\n",
    "        test_losses.append(avg_test_loss)\n",
    "        test_accuracies_sentiment.append(avg_accuracy_sentiment)\n",
    "        test_accuracies_engagement.append(avg_accuracy_engagement)\n",
    "\n",
    "        print(\n",
    "            f'Epoch {epoch+1}, Train Loss: {train_losses[-1]}, Test Loss: {avg_test_loss}, Test Accuracy Sentiment: {avg_accuracy_sentiment}, Test Accuracy Engagement: {avg_accuracy_engagement}')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed Learning Rate Multi Task Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-5\n",
    "num_epochs = 10\n",
    "\n",
    "model = Transformer(src_vocab_size=vocab_size, \n",
    "                    embed_size=128,\n",
    "                    num_layers=4,\n",
    "                    heads=8,\n",
    "                    forward_expansion=4,\n",
    "                    dropout=0.1,\n",
    "                    max_length=max_length,\n",
    "                    num_classes_sentiment=3,\n",
    "                    num_classes_engagement=3\n",
    "                )\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2819/2819 [00:55<00:00, 50.62it/s]\n",
      "100%|██████████| 705/705 [00:04<00:00, 146.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.03592141743514422, Test Loss: 1.0685446591241985, Test Accuracy Sentiment: 0.6386820673942566, Test Accuracy Engagement: 0.9207988977432251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2819/2819 [00:55<00:00, 50.84it/s]\n",
      "100%|██████████| 705/705 [00:04<00:00, 147.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.031948474652461784, Test Loss: 0.9563935298446222, Test Accuracy Sentiment: 0.7190209031105042, Test Accuracy Engagement: 0.9244336485862732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2819/2819 [00:56<00:00, 50.16it/s]\n",
      "100%|██████████| 705/705 [00:04<00:00, 141.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.029664756139168816, Test Loss: 0.9200128873612018, Test Accuracy Sentiment: 0.7487834692001343, Test Accuracy Engagement: 0.9254727959632874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2819/2819 [00:56<00:00, 50.24it/s]\n",
      "100%|██████████| 705/705 [00:04<00:00, 146.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.02818497419416783, Test Loss: 0.902097405736328, Test Accuracy Sentiment: 0.752058744430542, Test Accuracy Engagement: 0.9253989458084106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2819/2819 [00:56<00:00, 49.70it/s]\n",
      "100%|██████████| 705/705 [00:04<00:00, 146.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.027474813145105116, Test Loss: 0.8676464684888826, Test Accuracy Sentiment: 0.7630712985992432, Test Accuracy Engagement: 0.9251871705055237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2819/2819 [00:58<00:00, 47.99it/s]\n",
      "100%|██████████| 705/705 [00:05<00:00, 136.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.02700981883611208, Test Loss: 0.8810238599354494, Test Accuracy Sentiment: 0.7582692503929138, Test Accuracy Engagement: 0.9284130930900574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2819/2819 [00:57<00:00, 48.85it/s]\n",
      "100%|██████████| 705/705 [00:05<00:00, 140.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.02667090955817328, Test Loss: 0.8448654973337836, Test Accuracy Sentiment: 0.7703063488006592, Test Accuracy Engagement: 0.9281570315361023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2819/2819 [01:01<00:00, 46.12it/s]\n",
      "100%|██████████| 705/705 [00:05<00:00, 121.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.026295042641782763, Test Loss: 0.8530412191617573, Test Accuracy Sentiment: 0.7711091637611389, Test Accuracy Engagement: 0.9292996525764465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2819/2819 [01:02<00:00, 45.30it/s]\n",
      "100%|██████████| 705/705 [00:05<00:00, 135.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.026012080003216878, Test Loss: 0.8398456337604117, Test Accuracy Sentiment: 0.77036052942276, Test Accuracy Engagement: 0.9297626614570618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2819/2819 [00:59<00:00, 47.01it/s]\n",
      "100%|██████████| 705/705 [00:04<00:00, 145.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.025752227959278198, Test Loss: 0.8264187521968328, Test Accuracy Sentiment: 0.7827373743057251, Test Accuracy Engagement: 0.9242316484451294\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, train_loader, test_loader, num_epochs, optimizer)\n",
    "export_model(model,'transformer_model_with_fixed_learning_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Training Considerations and Transfer Learning Strategy\n",
    "\n",
    "When training a multi-task learning model like the Sentence Transformer adapted for tasks such as sentiment analysis and engagement prediction, several training strategies can be employed. Each has implications on the model's learning dynamics and performance:\n",
    "\n",
    "## Scenario 1: Freezing the Entire Network\n",
    "- **Implications:** Freezing the entire network means that all the weights are kept constant, and no learning occurs during training. This scenario is typically used when you apply a pre-trained model directly to a new task without any fine-tuning. It assumes the pre-trained weights are optimal for the new tasks without any adjustments.\n",
    "- **Advantages:** The main advantage is computational efficiency; no backpropagation is needed, and the model serves purely as a feature extractor. This can be useful in highly resource-constrained environments or when the pre-trained model is exceptionally well-aligned with the new tasks.\n",
    "- **Rationale:** Freezing the entire network is generally not recommended unless the new tasks are very similar to the tasks on which the model was originally trained. The lack of adaptability can lead to suboptimal performance if the tasks differ significantly.\n",
    "\n",
    "## Scenario 2: Freezing Only the Transformer Backbone\n",
    "- **Implications:** In this scenario, the shared transformer layers are frozen, and only the task-specific heads are trainable. This approach assumes that the shared layers already capture universal language features effectively and that only the final task-specific adaptations need learning.\n",
    "- **Advantages:** This method balances the benefits of transfer learning with the flexibility of task-specific tuning. It can lead to faster training and lower risk of overfitting the shared layers while allowing the model to adapt to the specifics of each task through the trainable heads.\n",
    "- **Rationale:** Freezing the backbone while training the heads is suitable when the pre-trained model's general features are relevant to the new tasks, but some adaptation is still required to optimize performance on specific task metrics.\n",
    "\n",
    "## Scenario 3: Freezing Only One of the Task-Specific Heads\n",
    "- **Implications:** Freezing one task-specific head while training the other allows for asymmetric learning where one task is considered stable or less important to optimize than the other. This might be used when one task is already performing at acceptable levels with pre-trained settings.\n",
    "- **Advantages:** This selective training focuses computational resources and model capacity on improving where it is most needed, potentially enhancing performance on a more challenging or impactful task without disturbing a satisfactory performance on another.\n",
    "- **Rationale:** Such a strategy would be adopted in a situation where improving performance on one task can lead to significant business or operational gains, while changes in the other are less beneficial or might even risk destabilizing established functionalities.\n",
    "\n",
    "## Transfer Learning Strategy\n",
    "When implementing a transfer learning strategy with a pre-trained model, consider the following steps:\n",
    "\n",
    "1. **Choice of a Pre-trained Model:**\n",
    "   - Select a model that has been trained on a large, comprehensive dataset similar to the tasks at hand, such as BERT or RoBERTa, which are trained on vast amounts of general text and are capable of understanding complex language patterns.\n",
    "\n",
    "2. **Layers to Freeze/Unfreeze:**\n",
    "   - **Freeze Early Layers:** Typically, earlier layers in transformer models capture more general linguistic features (e.g., syntax and common semantics), which are usually beneficial across different tasks and domains.\n",
    "   - **Unfreeze Later Layers:** Later layers, especially those closer to the output, tend to capture more task-specific features. Unfreezing these allows the model to adapt these layers to the specifics of the new tasks.\n",
    "\n",
    "3. **Rationale Behind Choices:**\n",
    "   - **Preserve General Features:** By freezing the early layers, the model preserves the robust features learned from large-scale data, reducing the risk of forgetting essential language understanding capabilities.\n",
    "   - **Adapt to Specific Tasks:** Unfreezing the later layers and the task-specific heads allows the model to adapt to the nuances of the specific tasks it is being fine-tuned for, improving its relevance and effectiveness on these tasks.\n",
    "\n",
    "Implementing these considerations and strategies ensures that the model benefits from the strengths of the pre-trained weights while still adapting sufficiently to excel at the new tasks. This approach optimizes the use of computational resources, enhances model performance, and mitigates the risks associated with overfitting and catastrophic forgetting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Layer-wise Learning Rate Implementation\n",
    "\n",
    "Implementing layer-wise learning rates in training deep neural networks is a sophisticated technique that tailors the learning process to the specifics of each layer's role within the model. This approach can optimize training dynamics, leading to more effective and efficient learning. Here's a deeper explanation of why different learning rates were set for each layer in the context of a multi-task sentence transformer model:\n",
    "\n",
    "## Rationale for Layer-wise Learning Rates\n",
    "\n",
    "### 1. **Base Learning Rate for Embeddings and Transformer Blocks:**\n",
    "   - **Rate:** `base_lr = 3e-6`\n",
    "   - **Reason:** The embedding layer and the transformer blocks form the foundation of the model, capturing general linguistic and contextual information from the input text. These layers are typically pre-trained on large datasets and are highly sensitive. A lower learning rate is used here to make fine, cautious adjustments, preserving the rich, pre-trained features while preventing drastic changes that might lead to forgetting useful information.\n",
    "\n",
    "### 2. **Learning Rate for Sentiment Classifier:**\n",
    "   - **Rate:** `sentiment_classifier_lr = 3e-5`\n",
    "   - **Reason:** The sentiment classifier tailors the output of the shared transformer architecture to a specific task — sentiment analysis. A higher learning rate compared to the base layers allows this classifier to quickly adapt to the nuances of sentiment classification. However, it was noted that the sentiment classifier's accuracy was initially low, suggesting that the task might be more complex or that the initial parameters were not optimal. A slightly conservative rate (relative to the engagement classifier) was therefore chosen to facilitate more stable and gradual learning, enhancing its ability to refine its parameters without drastic oscillations.\n",
    "\n",
    "\n",
    "### 3. **Learning Rate for Engagement Classifier:**\n",
    "   - **Rate:** `engagement_classifier_lr = 3e-4`\n",
    "   - **Reason:** Engagement prediction might be somewhat less complex or differently characterized compared to sentiment analysis, or it might benefit from more aggressive updates due to different initial performance baselines. Therefore, a higher learning rate is employed to enable faster learning adjustments, allowing the model to quickly optimize its predictions based on engagement-specific feedback.\n",
    "\n",
    "## Benefits of Layer-wise Learning Rates in Multi-task Settings\n",
    "\n",
    "### Enhanced Task-specific Adaptation:\n",
    "   - **Multi-task Efficiency:** By using different learning rates, each part of the model can learn at a pace suitable for its specific task and complexity level. This is particularly beneficial in a multi-task setting where different tasks may have varying degrees of difficulty and data characteristics.\n",
    "   - **Prevents Overfitting:** Lower rates in foundational layers help prevent overfitting by ensuring that these layers, which are responsible for capturing universal features, do not change too rapidly. This stability is crucial when the model is applied across multiple tasks that might pull the foundational layers in different directions.\n",
    "   - **Encourages Task-specific Fine-tuning:** Higher rates in the task-specific layers encourage these layers to fine-tune aggressively to their respective tasks, making the model more responsive to task-specific signals without affecting the shared layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer wise Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (word_embedding): Embedding(30000, 128)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (values): Linear(in_features=16, out_features=16, bias=False)\n",
       "        (keys): Linear(in_features=16, out_features=16, bias=False)\n",
       "        (queries): Linear(in_features=16, out_features=16, bias=False)\n",
       "        (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (sentiment_classifier): Linear(in_features=128, out_features=3, bias=True)\n",
       "  (engagement_classifier): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_lr = 3e-6  # Lower learning rate for embeddings and transformer blocks\n",
    "sentiment_classifier_lr = 3e-5  # Higher learning rate for classifiers\n",
    "engagement_classifier_lr = 3e-4  # Higher learning rate for classifiers\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.word_embedding.parameters(), 'lr': base_lr},\n",
    "    {'params': model.positional_encoding.parameters(), 'lr': base_lr},\n",
    "    {'params': [p for layer in model.layers for p in layer.parameters()], 'lr': base_lr},\n",
    "    {'params': model.sentiment_classifier.parameters(), 'lr': sentiment_classifier_lr},\n",
    "    {'params': model.engagement_classifier.parameters(), 'lr': engagement_classifier_lr}\n",
    "])\n",
    "\n",
    "\n",
    "model = Transformer(src_vocab_size=vocab_size, \n",
    "                    embed_size=128,\n",
    "                    num_layers=4,\n",
    "                    heads=8,\n",
    "                    forward_expansion=4,\n",
    "                    dropout=0.1,\n",
    "                    max_length=max_length,\n",
    "                    num_classes_sentiment=3,\n",
    "                    num_classes_engagement=3\n",
    "                )\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2819/2819 [01:02<00:00, 45.05it/s]\n",
      "100%|██████████| 705/705 [00:05<00:00, 120.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.025468018559272547, Test Loss: 0.804600294874915, Test Accuracy Sentiment: 0.7830624580383301, Test Accuracy Engagement: 0.9308067560195923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2819/2819 [01:05<00:00, 43.14it/s]\n",
      "100%|██████████| 705/705 [00:05<00:00, 131.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.025266446947131448, Test Loss: 0.8190241541845579, Test Accuracy Sentiment: 0.7803486585617065, Test Accuracy Engagement: 0.9311367869377136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2819/2819 [01:03<00:00, 44.36it/s]\n",
      "100%|██████████| 705/705 [00:05<00:00, 127.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.025009157959757455, Test Loss: 0.8058740529819584, Test Accuracy Sentiment: 0.784727156162262, Test Accuracy Engagement: 0.9278663992881775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2819/2819 [01:03<00:00, 44.22it/s]\n",
      "100%|██████████| 705/705 [00:05<00:00, 131.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.024882146644341664, Test Loss: 0.8122115374245542, Test Accuracy Sentiment: 0.7832298874855042, Test Accuracy Engagement: 0.9313485026359558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2819/2819 [01:04<00:00, 43.71it/s]\n",
      "100%|██████████| 705/705 [00:05<00:00, 139.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.024684242282588535, Test Loss: 0.7910871576332877, Test Accuracy Sentiment: 0.7903122901916504, Test Accuracy Engagement: 0.9303634762763977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2819/2819 [00:59<00:00, 47.22it/s]\n",
      "100%|██████████| 705/705 [00:04<00:00, 152.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.024463480261291533, Test Loss: 0.8043776453809536, Test Accuracy Sentiment: 0.7868301868438721, Test Accuracy Engagement: 0.9278122782707214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2819/2819 [01:00<00:00, 46.64it/s]\n",
      "100%|██████████| 705/705 [00:05<00:00, 120.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.02433282113751777, Test Loss: 0.7815935524642891, Test Accuracy Sentiment: 0.7929028868675232, Test Accuracy Engagement: 0.9290090799331665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2819/2819 [01:01<00:00, 45.57it/s]\n",
      "100%|██████████| 705/705 [00:05<00:00, 136.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.02423191926319238, Test Loss: 0.7823284150438106, Test Accuracy Sentiment: 0.7922379970550537, Test Accuracy Engagement: 0.9280585050582886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2819/2819 [01:00<00:00, 46.66it/s]\n",
      "100%|██████████| 705/705 [00:06<00:00, 112.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.024080614274081674, Test Loss: 0.7824346129564529, Test Accuracy Sentiment: 0.7927156686782837, Test Accuracy Engagement: 0.9303634762763977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2819/2819 [01:26<00:00, 32.76it/s]\n",
      "100%|██████████| 705/705 [00:04<00:00, 150.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.023964314545471453, Test Loss: 0.7993287307573549, Test Accuracy Sentiment: 0.7912726402282715, Test Accuracy Engagement: 0.9278811812400818\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, train_loader, test_loader, num_epochs, optimizer)  \n",
    "export_model(model,'transformer_model_with_layer_wise_learning_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
